# Simple-LLM-App
Building a simple LLM application
This LLM application gives a batch response. 
The model relies on certain inputs to function properly (specific model, temperature, message, system prompt and output type)
The front-end streamlit (main.py) sends a post request to the backend engine (app.py).
